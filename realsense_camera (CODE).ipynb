{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines two classes, RealsenseCamera and MaskRCNN.\n",
    "\n",
    "RealsenseCamera is a class for controlling a RealSense camera and retrieving its color and depth frames. It has the following methods:\n",
    "\n",
    "    __init__(self): initializes the camera, sets its color and depth streams, and starts streaming.\n",
    "\n",
    "    get_frame_stream(self): retrieves the latest color and depth frames from the camera, applies filters to the depth frame to fill holes, generates a depth color map, and returns the color and depth frames as numpy arrays.\n",
    "\n",
    "    release(self): stops the camera streaming.\n",
    "\n",
    "MaskRCNN is a class for performing object detection using the Mask R-CNN algorithm. It has the following methods:\n",
    "\n",
    "    __init__(self): loads the Mask R-CNN model and sets its preferences, initializes a set of random colors for each class, reads the class names from a text file, and initializes lists for storing the detected objects' bounding boxes, class IDs, centers, and contours, as well as a list for storing the distances to each detected object.\n",
    "\n",
    "    detect_objects_mask(self, bgr_frame): takes a BGR frame as input, preprocesses it, runs it through the Mask R-CNN model, retrieves the detected object bounding boxes, class IDs, and masks, and stores them in the previously initialized lists. It returns nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code defines a class RealsenseCamera that interfaces with an Intel Realsense camera to obtain RGB and depth images.\n",
    "\n",
    "# First, we import the pyrealsense2 module and numpy.\n",
    "# The RealsenseCamera class has an initializer method (__init__) that creates a pipeline and configures the depth and color streams. The pipeline is started and the alignment is defined.\n",
    "# The get_frame_stream() method waits for a pair of coherent frames, depth and color, from the pipeline. The frames are aligned and filtered using the hole filling filter. The depth frame is converted into a depth colormap and the color and depth frames are converted into numpy arrays.\n",
    "# The release() method stops the pipeline.\n",
    "\n",
    "\n",
    "# Overall, this class provides a convenient interface for accessing RGB and depth images from an Intel Realsense camera, with hole filling and alignment already implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RealsenseCamera:\n",
    "    def __init__(self):\n",
    "        # Configure depth and color streams\n",
    "        print(\"Loading Intel Realsense Camera\")\n",
    "        self.pipeline = rs.pipeline()\n",
    "\n",
    "        config = rs.config()\n",
    "        config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "        config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n",
    "\n",
    "        # Start streaming\n",
    "        self.pipeline.start(config)\n",
    "        align_to = rs.stream.color\n",
    "        self.align = rs.align(align_to)\n",
    "\n",
    "\n",
    "    def get_frame_stream(self):\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "        aligned_frames = self.align.process(frames)\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            # If there is no frame, probably camera not connected, return False\n",
    "            print(\"Error, impossible to get the frame, make sure that the Intel Realsense camera is correctly connected\")\n",
    "            return False, None, None\n",
    "        \n",
    "        # Apply filter to fill the Holes in the depth image\n",
    "        spatial = rs.spatial_filter()\n",
    "        spatial.set_option(rs.option.holes_fill, 3)\n",
    "        filtered_depth = spatial.process(depth_frame)\n",
    "\n",
    "        hole_filling = rs.hole_filling_filter()\n",
    "        filled_depth = hole_filling.process(filtered_depth)\n",
    "\n",
    "        \n",
    "        # Create colormap to show the depth of the Objects\n",
    "        colorizer = rs.colorizer()\n",
    "        depth_colormap = np.asanyarray(colorizer.colorize(filled_depth).get_data())\n",
    "\n",
    "        \n",
    "        # Convert images to numpy arrays\n",
    "        # distance = depth_frame.get_distance(int(50),int(50))\n",
    "        # print(\"distance\", distance)\n",
    "        depth_image = np.asanyarray(filled_depth.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # cv2.imshow(\"Colormap\", depth_colormap)\n",
    "        # cv2.imshow(\"depth img\", depth_image)\n",
    "\n",
    "        return True, color_image, depth_image\n",
    "    \n",
    "    def release(self):\n",
    "        self.pipeline.stop()\n",
    "        #print(depth_image)\n",
    "        \n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        #depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.10), 2)\n",
    "\n",
    "        # Stack both images horizontally\n",
    "        \n",
    "        #images = np.hstack((color_image, depth_colormap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Intel Realsense Camera\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load Realsense camera\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m rs \u001b[38;5;241m=\u001b[39m \u001b[43mRealsenseCamera\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m mrcnn \u001b[38;5;241m=\u001b[39m MaskRCNN()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m \t\u001b[38;5;66;03m# Get frame in real time from Realsense camera\u001b[39;00m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mRealsenseCamera.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m config\u001b[38;5;241m.\u001b[39menable_stream(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mdepth, \u001b[38;5;241m1280\u001b[39m, \u001b[38;5;241m720\u001b[39m, rs\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mz16, \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Start streaming\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m align_to \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mcolor\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malign \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39malign(align_to)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No device connected"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# Load Realsense camera\n",
    "rs = RealsenseCamera()\n",
    "mrcnn = MaskRCNN()\n",
    "\n",
    "while True:\n",
    "\t# Get frame in real time from Realsense camera\n",
    "\tret, bgr_frame, depth_frame = rs.get_frame_stream()\n",
    "\n",
    "\t# Get object mask\n",
    "\tboxes, classes, contours, centers = mrcnn.detect_objects_mask(bgr_frame)\n",
    "\n",
    "\t# Draw object mask\n",
    "\tbgr_frame = mrcnn.draw_object_mask(bgr_frame)\n",
    "\n",
    "\t# Show depth info of the objects\n",
    "\tmrcnn.draw_object_info(bgr_frame, depth_frame)\n",
    "\n",
    "\n",
    "\tcv2.imshow(\"depth frame\", depth_frame)\n",
    "\tcv2.imshow(\"Bgr frame\", bgr_frame)\n",
    "\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "rs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class MaskRCNN:\n",
    "    def __init__(self):\n",
    "        # Loading Mask RCNN\n",
    "        self.net = cv2.dnn.readNetFromTensorflow(\"dnn/frozen_inference_graph_coco.pb\",\n",
    "                                            \"dnn/mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\")\n",
    "        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "        # Generate random colors\n",
    "        np.random.seed(2)\n",
    "        self.colors = np.random.randint(0, 255, (90, 3))\n",
    "\n",
    "        # Conf threshold\n",
    "        self.detection_threshold = 0.7\n",
    "        self.mask_threshold = 0.3\n",
    "\n",
    "        self.classes = []\n",
    "        with open(\"dnn/classes.txt\", \"r\") as file_object:\n",
    "            for class_name in file_object.readlines():\n",
    "                class_name = class_name.strip()\n",
    "                self.classes.append(class_name)\n",
    "\n",
    "        self.obj_boxes = []\n",
    "        self.obj_classes = []\n",
    "        self.obj_centers = []\n",
    "        self.obj_contours = []\n",
    "\n",
    "        # Distances\n",
    "        self.distances = []\n",
    "\n",
    "\n",
    "    def detect_objects_mask(self, bgr_frame):\n",
    "        blob = cv2.dnn.blobFromImage(bgr_frame, swapRB=True)\n",
    "        self.net.setInput(blob)\n",
    "\n",
    "        boxes, masks = self.net.forward([\"detection_out_final\", \"detection_masks\"])\n",
    "\n",
    "        # Detect objects\n",
    "        frame_height, frame_width, _ = bgr_frame.shape\n",
    "        detection_count = boxes.shape[2]\n",
    "\n",
    "        # Object Boxes\n",
    "        self.obj_boxes = []\n",
    "        self.obj_classes = []\n",
    "        self.obj_centers = []\n",
    "        self.obj_contours = []\n",
    "\n",
    "        for i in range(detection_count):\n",
    "            box = boxes[0, 0, i]\n",
    "            class_id = box[1]\n",
    "            score = box[2]\n",
    "            color = self.colors[int(class_id)]\n",
    "            if score < self.detection_threshold:\n",
    "                continue\n",
    "\n",
    "            # Get box Coordinates\n",
    "            x = int(box[3] * frame_width)\n",
    "            y = int(box[4] * frame_height)\n",
    "            x2 = int(box[5] * frame_width)\n",
    "            y2 = int(box[6] * frame_height)\n",
    "            self.obj_boxes.append([x, y, x2, y2])\n",
    "\n",
    "            cx = (x + x2) // 2\n",
    "            cy = (y + y2) // 2\n",
    "            self.obj_centers.append((cx, cy))\n",
    "\n",
    "            # append class\n",
    "            self.obj_classes.append(class_id)\n",
    "\n",
    "            # Contours\n",
    "            # Get mask coordinates\n",
    "            # Get the mask\n",
    "            mask = masks[i, int(class_id)]\n",
    "            roi_height, roi_width = y2 - y, x2 - x\n",
    "            mask = cv2.resize(mask, (roi_width, roi_height))\n",
    "            _, mask = cv2.threshold(mask, self.mask_threshold, 255, cv2.THRESH_BINARY)\n",
    "            contours, _ = cv2.findContours(np.array(mask, np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            self.obj_contours.append(contours)\n",
    "\n",
    "        return self.obj_boxes, self.obj_classes, self.obj_contours, self.obj_centers\n",
    "\n",
    "    def draw_object_mask(self, bgr_frame):\n",
    "        # loop through the detection\n",
    "        for box, class_id, contours in zip(self.obj_boxes, self.obj_classes, self.obj_contours):\n",
    "            x, y, x2, y2 = box\n",
    "            roi = bgr_frame[y: y2, x: x2]\n",
    "            roi_height, roi_width, _ = roi.shape\n",
    "            color = self.colors[int(class_id)]\n",
    "\n",
    "            roi_copy = np.zeros_like(roi)\n",
    "\n",
    "            for cnt in contours:\n",
    "                # cv2.f(roi, [cnt], (int(color[0]), int(color[1]), int(color[2])))\n",
    "                cv2.drawContours(roi, [cnt], - 1, (int(color[0]), int(color[1]), int(color[2])), 3)\n",
    "                cv2.fillPoly(roi_copy, [cnt], (int(color[0]), int(color[1]), int(color[2])))\n",
    "                roi = cv2.addWeighted(roi, 1, roi_copy, 0.5, 0.0)\n",
    "                bgr_frame[y: y2, x: x2] = roi\n",
    "        return bgr_frame\n",
    "\n",
    "    def draw_object_info(self, bgr_frame, depth_frame):\n",
    "        # loop through the detection\n",
    "        for box, class_id, obj_center in zip(self.obj_boxes, self.obj_classes, self.obj_centers):\n",
    "            x, y, x2, y2 = box\n",
    "\n",
    "            color = self.colors[int(class_id)]\n",
    "            color = (int(color[0]), int(color[1]), int(color[2]))\n",
    "\n",
    "            cx, cy = obj_center\n",
    "\n",
    "            depth_mm = depth_frame[cy, cx]\n",
    "\n",
    "            cv2.line(bgr_frame, (cx, y), (cx, y2), color, 1)\n",
    "            cv2.line(bgr_frame, (x, cy), (x2, cy), color, 1)\n",
    "\n",
    "            class_name = self.classes[int(class_id)]\n",
    "            cv2.rectangle(bgr_frame, (x, y), (x + 250, y + 70), color, -1)\n",
    "            cv2.putText(bgr_frame, class_name.capitalize(), (x + 5, y + 25), 0, 0.8, (255, 255, 255), 2)\n",
    "            cv2.putText(bgr_frame, \"{} cm\".format(depth_mm / 10), (x + 5, y + 60), 0, 1.0, (255, 255, 255), 2)\n",
    "            cv2.rectangle(bgr_frame, (x, y), (x2, y2), color, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return bgr_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
